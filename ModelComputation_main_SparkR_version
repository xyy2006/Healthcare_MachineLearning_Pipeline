#!/usr/bin/env Rscript
## !/usr/bin/Rscript --vanilla
##!/cadappl/R/3.1.0/lib64/R/bin/Rscript --vanilla --slave
# Content: setting up the env for parallel computing and every variables needed for further computation using a specified statistical model/module.
# Author: Yang Yang
# Date:"Tue Jul  1 08:57:26 2014"
# Usage: executable.r -h
# Note: To run this program, you need at least prepare the input Rdata put under ./ (with all response variables and covariates, a big matrix-like object), factor_details.txt file (put under ./Factors_details_file/), and optionally the MultipleNodes_profile.txt put under ./Main_file/. Filenames can be any, as you can specify in below cmd line arguments.

# example:
# ---full run--- #
#		./ModelComputation_main -i NY_SID_2009_HFdata_refined.Rdata -F factor_details_forLM.txt -f uniCorr -p RSForest -c uniCorr_model.txt -e RSForest_model.txt -o RSForest -P SNOW -j FALSE
# ---demo run on single node--- #
#		./ModelComputation_main -i NY_SID_2009_HFdata_refined_subsetDemo.Rdata -F factor_details_forLM.txt -f uniCorr -p RSForest -c uniCorr_model.txt -e RSForest_model.txt -o RSForest -P SNOW -j FALSE
# ---demo run on multiple nodes--- #
#		./ModelComputation_main -i NY_SID_2009_HFdata_refined_subsetDemo.Rdata -F factor_details_forLM.txt -f uniCorr -p RSForest -c uniCorr_model.txt -e RSForest_model.txt -o RSForest -P SNOW -j T -m MultipleNodes_profile.txt
# ---demo run on multiple nodes for grpreg model.--- #
#		./ModelComputation_main -i NY_SID_2009_HFdata_refined_subsetDemo.Rdata -F factor_details_forLM.txt -f uniCorr -p grpreg_linearGroupRegularized -c uniCorr_model.txt -e grpreg_linearGroupRegularized_model.txt -o grpreg -P SNOW -j FALSE
# ---demo run on multiple nodes for glmnet model.--- #
#		./ModelComputation_main -i NY_SID_2009_HFdata_refined_subsetDemo.Rdata -F factor_details_forLM.txt -f uniCorr -p glmnet_linearRegularized -c uniCorr_model.txt -e glmnet_linearRegularized_model.txt -o glmnet -P SNOW -j T -m MultipleNodes_profile.txt
# ---demo run on multiple nodes for Survival_glmnet model.--- #
#		./ModelComputation_main -i NY_SID_2009_HFdata_refined_subsetDemo.Rdata -F factor_details_forLM.txt -f uniCorr -p Survival_glmnet -c uniCorr_model.txt -e Survival_glmnet_model.txt -o Survival_glmnet -P SNOW -j T -m MultipleNodes_profile.txt
#####################################################################################
suppressPackageStartupMessages(require(getopt)) # load necessary packages
# 0, require no args; 1, require args; 2, optional, require args.
spec = matrix(c(
  'verbose', 'v', 0, "logical","display verbose message printed",
  'help'   , 'h', 0, "logical","call 'help' document",
  'input_file'  , 'i', 1, "character","the data you want to run program on, must be one of .Rdata, .RDS, .text, .txt, .cvs)",
  'i_f_delim','d',2,'character',"[optional] if you provide text input rather than R binary data, specify the delimiter, current support either [t] or [,] (please not include the square parentheses). [t]: tab delimited; [,]: comma delimited. If you dont specify, program will decide automatically, which may not be correct.",
  'output_folder_name','O',2,'character',"[optional] if you provide the output folder name, there will be a new folder, e.g. './new_folder', made for output files; otherwise, output will go to default folder './Output_file'. Note: log file is always put under program root pwd for convenience.",
  'factors_details_file', 'F', 1, "character","a filename put in the same folder as program, which contains columns (delimited by comma/tab/space) like this (for details check example file): \n \t\t\t\t\t\tage, knowledge, continuous\n \t\t\t\t\t\tgender, knowledge, categorical\n \t\t\t\t\t\tDXCCS1, data, categorical\n \t\t\t\t\t\tNDX, data, continuous",
  'PrefeatureSelection_module','f',2,'character','[optional] which statistical model/module you want to use for feature selection. It must be one from infoGain,LAR,fisherScore,uniCorr,etc.',
  'Predictive_module','p',1,'character','which statistical model/module you want to use for prediction. It must be one from linearModelStepwise, RSForest,etc.',
  'Pf_m_config_file','c',2,'character','the configuration filename for PrefeatureSelection_module you choose, within which you can tune arguments for that specified module. If omitted, use defaults',
  'p_m_config_file','e',2,'character','the configuration filename for Predictive_module you choose, within which you can tune arguments for that specified module. If omitted, use defaults',
  'prefix', 'o', 1, "character","the prefix you want to add to any output file names, e.g. 'Prefix123_regressionOBJ.Rdata'",
  'parallel_scheme', 'P', 1, "character","current support: 'SERIAL','SNOW' or 'MC'. 'SERIAL' forces run in serial (e.g. it is important to use 'SERIAL' when migrate the code to hadoop system, etc.)",
  'jobExecuteOnMultipleNodes', 'j', 1, "logical","'TRUE' or 'FALSE' (with 'TRUE' means parallel run on multiple nodes (force use 'SNOW'), which are more complicated and need pre-configuration of the cluster environment. You may need to consult the cluster administrator before you do that way.)",
  'MultipleNodes_profile','m',2,'character',"This options comes together with 'jobExecuteOnMultipleNodes'==TRUE. The file contains the configuration for the virture cluster with multiple nodes you intent to use. Check the example file.",
  'kFoldCV','k',2,'integer',"By specifying 'k', you enable manual CV in the pipeline for pre-feature selection module and predictive module. 'k' is a integer representing k in k-fold cross validation. For example, use k = 5 will split the data into 5 bins with each time using 4 bins for training, 1 bins for validating purpose. Some predictive module has within-package kFoldCV/or OOB algorithm, e.g. RSForest, glmnet/ncvreg/grpreg-based modules, etc. Therefore, avoiding enabling this option in such cases.",
  'ProportionHoldingTestDataset','H',2,'double',"a proportion value indicating how much data you want to pre-hold for testing dataset, e.g. 0.3 means 30% of the total dataset will be kept out solely for testing purpose, then 70% dataset will be used for training or k-fold CV training.",
  'seed','s',2,'integer',"since sampling/splitting the data into training/validation/testing data portion is by random sampling, giving a seed will make the split deterministic, i.e. same seed will lead to same data split."
), byrow=TRUE, ncol=5)
# 'response_variables', 'y', 1, "character","colnames delimited by comma/tab/space. E.g., LOS.x,DEATH",
#'knowledge_driven_factors'  , 'K', 1, "character","colnames delimited by comma/tab/space. E.g., AGE,GENDER,WEIGHT",
#'data_driven_factors', 'D', 1, "character","colnames delimited by comma/tab/space. E.g., PRCCS3, PRCCS4, PRCCS5",

opt = getopt(spec);
print(opt)
# if help was asked for print a friendly message
# and exit with a non-zero error code
if ( length(opt) == 1 | !is.null(opt$help) ) { # conditions call 'help'
  cat(getopt(spec, usage=TRUE));
  suppressPackageStartupMessages( invisible(source( sprintf("./Main_file/extract_moduleList_from_ModulesCatalog.r") ) ) )	
  Modules_Catalog_file <- "./Module_file/Modules_Catalog.txt"
  message("#====================================================#")
  message("Current available algorithms for:")
  algorithmListForModules <- extract_moduleList_from_ModulesCatalog(Modules_Catalog_file)	# will print current available algorithms for each module.
  q(status=1);
}

# #####################################
# ## for debugging purpose, manually assigning values to the specs above.
# input_file <- "NY_SID_2009_HFdata_refined.Rdata"	# later we can do format-adaptive loadin
# input_file_delimiter <- NULL
# featureSelection_module <- "uniCorr"
# predictive_module <- "RSForest"
# Pf_m_config_file <- "Rscript/uniCorr_model.txt"
# p_m_config_file <- "Rscript/RSForest_model.txt"
# response_variables <- c("LOS.x","died")
# factors_details_file <- "factor_details_forLM.txt"
# parallel_scheme <- "MC"
# jobExecuteOnMultipleNodes <- "FALSE"
# MultipleNodes_profile <- "MultipleNodes_profile.txt"
# kFoldCV <- 5
# ProportionHoldingTestDataset <- 0.3
# seed = 123
# #####################################
#set some reasonable defaults for the options that are needed,
#but were not specified.
# if ( is.null(opt$mean    ) ) { opt$mean    = 0     }
# if ( is.null(opt$sd      ) ) { opt$sd      = 1     }
# if ( is.null(opt$count   ) ) { opt$count   = 10    }
# if ( is.null(opt$verbose ) ) { opt$verbose = FALSE }
#print some progress messages to stderr, if requested.
if ( !is.null(opt$verbose) ) { write("Parsing commands...",stdout()); }
#
for(i in 1:length(opt))  {
  assign(names(opt)[i],opt[[i]]) # assign opt member obj into .GlobalEnv for later call usage.
}
#---------#
# set option to allow error dumpping. The dumping error file will always show up in user specified output folder after error occurs.
programRootPWD <-  getwd() 	# for later usage of root path.
invisible( ifelse(exists("output_folder_name"), yes = {dir.create(output_folder_name);setwd(output_folder_name)}, no = {dir.create("Output_file");setwd("Output_file")} ) )	# dont print the pwd which is the default behaviour of setwd.
outputPWD <- getwd()	# now program are already at outputPWD.
#
if ( exists("prefix") ) {
  options(error = quote({
    message("========================================================" );
    message("Please check log file under program root and error dump file in", outputPWD );
    dump.frames(dumpto = sprintf("%s/%s.last.dump",outputPWD,prefix),to.file = TRUE); 
    # file.show(sprintf("%s/%s_riskModelling.log",programRootPWD,prefix));  # alternatively, show the file bay Bash shell cmd after R exit.
    q()}))	# for debugging purpose, will output a file "$prefix.last.dump.rda" for further debugging if error occured.
} else 	{
  prefix <- "Default"	# give the default value.
  options(error = quote({
    message("========================================================" );
    message("Please check log file under program root and error dump file in", outputPWD );
    dump.frames(dumpto = sprintf("%s/%s.last.dump",outputPWD,prefix),to.file = TRUE); 
    # file.show(sprintf("%s/%s_riskModelling.log",programRootPWD,prefix));
    q()}))	# for debugging purpose, will output a file "last.dump.rda" for further debugging if error occured.
}
#---#
# transforming jobExecuteOnMultipleNodes to another variable of class "logical".
if ( grepl("^T",jobExecuteOnMultipleNodes,perl=T,ig=T) ) jEOM <- T else if ( grepl("^F",jobExecuteOnMultipleNodes,perl=T,ig=T) ) jEOM = F else {    
  cat(getopt(spec, usage=TRUE));
  cat('========================================================\n')
  stop("Something wrong with parallel_scheme appointment!")
  q(status=1);
}




#===================================================================================#
# start logging.
log_connection <- file(sprintf("%s/%s_riskModelling.log",programRootPWD,prefix), open = "wt")	# log file always put under './' deliberately (it's convenient after you run the program, you dont need to change folder to look at the log file.)
sink(log_connection, type="output",split = TRUE)
# sink(log_connection, type="message",split = FALSE)	# cannot split, so prefer print on screen for the errors.
# on.exit( catt(geterrmessage()) )	# only used in function.
cat("Start at ", date(),"\n" )
cat("parallel_scheme choice is:",parallel_scheme,"\n")
# the example we use is
cat("=====================================================\n")
suppressPackageStartupMessages( invisible(source( sprintf("%s/Main_file/head.r",programRootPWD) ) ) )	# the head import
suppressPackageStartupMessages( invisible(source(sprintf("%s/Main_file/extract_model_fromModelFile.r",programRootPWD) ) ) )	# the extract_model from model_file.
# suppressPackageStartupMessages( invisible(source("extract_model_fromModelFile.r") ) )	
# setwd("/home/310166442/HCUP_data")
# strsplit(knowledge_driven_factors,split=c(",","\\s") ,perl=T)
# strsplit(data_driven_factors,split=c(",",".") ,perl=T)
# #----------------setting up the parallel env------------------------#
###::: use "SparkR" env instead of HPC parallel protocols
source(sprintf("%s/Main_file/setting_up_sparkR_env.r", programRootPWD) )# so we have 'sc' as spark-cluster.

# # ---a subfunc to setup the parallel env---#
# setParallel <- function(parallel_scheme = parallel_scheme){
#   switch(parallel_scheme,
#          MC = registerDoMC(detectCores()),
#          SNOW = {cl <<- makeSOCKcluster(detectCores())	# make it mutable object.
#          setDefaultCluster(cl)
#          registerDoSNOW(cl)
#          #---need to send the path and point to the correct path on each slave node---
#          clusterExport(cl, "programRootPWD")
#          clusterEvalQ(cl,source( sprintf('%s/Main_file/head_forNonInteractive_SlaveNodes.r',programRootPWD) )) # load necessary packages on each, slave must be given absolute path, REMEBER!
#          # on.exit({stopCluster(cl)})	# turn if off savely, not put here, otherwise generate -> kill immediately.
#          },
#          SERIAL = registerDoSEQ() ,
#          stop("Not valid parallel_scheme appointed!")
#   )
#   #
#   catt("We have registered ",getDoParWorkers()," cpu(s) on machine: ",system("hostname",intern=T), " by ", shQuote(getDoParName()) )
# }
# #-------#
# if (!jEOM) {
#   setParallel(parallel_scheme)
#   on.exit({stopCluster(cl)})
# } else if (jEOM && exists("MultipleNodes_profile") ) {
#   m_n_profile <- read.table(sprintf("%s/Main_file/%s",programRootPWD,MultipleNodes_profile),header=T)
#   nodes_file <- unlist( apply(m_n_profile, 1, function(x) rep(x['nodeName'],x['cpus'])) )	# the cpus pool.
#   cl <- makeSOCKcluster(nodes_file)
#   setDefaultCluster(cl)
#   registerDoSNOW(cl)
#   #---need to send the path and point to the correct path on each slave node---
#   clusterExport(cl, "programRootPWD")
#   clusterEvalQ(cl,source( sprintf('%s/Main_file/head_forNonInteractive_SlaveNodes.r',programRootPWD) )) # load necessary packages on each, slave must be given absolute path, REMEBER!
#   on.exit({stopCluster(cl)})	# turn if off savely.
#   catt("We have registered ",getDoParWorkers()," cpu(s) by ", shQuote(getDoParName()), "on machine: " )
#   print(m_n_profile)
# } else	{
#   setParallel(parallel_scheme)	# still run on single node.
#   on.exit({stopCluster(cl)})
# }
# # clusterEvalQ(cl,Sys.info()['nodename'])
# # # not yet executed
# # # clusterEvalQ(cl,ls())	# for debugging purpose.
# catt("Again now we have ",getDoParWorkers(),  " cores running!")

# catt( unlist(clusterEvalQ(cl,Sys.info()['nodename'])) )	# for debugging purpose.
# #--------extract the variables we need for predictors from input----------------------#
# m<-gregexpr(",|\\s+",response_variables,perl=T)
# response_variables_vec <- unlist(regmatches(response_variables,m,invert=T) ) 
# #
# m<-gregexpr(",|\\s+",knowledge_driven_factors,perl=T)
# knowledge_driven_factors_vec <- unlist(regmatches(knowledge_driven_factors,m,invert=T) ) 
# #
# m<-gregexpr(",|\\s+",data_driven_factors,perl=T)
# data_driven_factors_vec <- unlist(regmatches(data_driven_factors,m,invert=T) )
# #---a sub func to exclude those like ""------#
# eliminate_nchar0 <- function(x) nchar(x)!=0
# #---#
# response_variables_vec <- response_variables_vec[sapply(response_variables_vec,eliminate_nchar0)]
# knowledge_driven_factors_vec <- knowledge_driven_factors_vec[ sapply(knowledge_driven_factors_vec,eliminate_nchar0)]
# data_driven_factors_vec <- data_driven_factors_vec[ sapply(data_driven_factors_vec,eliminate_nchar0)]
#--------------setting up the data for computing on-------------------------#
catt("Start loading input_file!")
suffix <- sub((".+?\\.(\\w+$)"),"\\1",input_file,perl=T) %>>% tolower
if (exists("i_f_delim") ) {	# normalize the file delimiter.
  i_f_delim <- switch(i_f_delim,
                      `t` = '\t',
                      `,` = ',',
                      stop("Please check the supported delimiter in program helper!")
  )
}
#---switch how to load input data---#
switch(suffix,
       txt = { data_input <- fread( sprintf("%s/%s",programRootPWD,input_file),header = TRUE, sep = ifelse(exists("i_f_delim"), yes = i_f_delim, no = "auto") )},
       
       csv = { data_input <- fread( sprintf("%s/%s",programRootPWD,input_file),header = TRUE, sep = ifelse(exists("i_f_delim"), yes = i_f_delim, no = "auto") )},
       
       text = { data_input <- fread( sprintf("%s/%s",programRootPWD,input_file),header = TRUE, sep = ifelse(exists("i_f_delim"), yes = i_f_delim, no = "auto") )},
       
       rds = {
         data_input <- readRDS( sprintf("%s/%s",programRootPWD,input_file) )
       },
       
       rdata = {
         temp_load <- load( sprintf("%s/%s",programRootPWD,input_file) )
         data_input <- eval(as.name(temp_load))
       },
       
       stop( sprintf("Please provide a valid input format for '%s' with txt/csv/text/Rdata suffix accordingly!", input_file) )	# currently report error for all other suffix.
)
if (!inherits(data_input,"data.table")) data_input <- as.data.table(data_input)	# data.table package is a must.
# a DT and DF object.
# data_input[,sapply(.SD,function(x) sum(is.na(x)))]
#---------------------------------------------------------------------------#
#read the factor file in
catt("Start factors_details_file!")
# factors_all <- fread(factors_details_file,skip=4)	# skip=4, can vary to skip comments.
factors_all <- fread(sprintf("%s/Factors_details_file/%s",programRootPWD,factors_details_file),skip=4)	# skip=4, can vary to skip comments.
factors_all <- factors_all[grep("no",includedInAnalysis,perl=T,ignore=T,invert=T),]
factors_all$drivenType = tolower(factors_all$drivenType)	# compatiblity.
factors_all$dataType = tolower(factors_all$dataType)

# factors_all[,sapply(.SD,class)]
# colname         drivenType           dataType includedInAnalysis
# "character"        "character"        "character"        "character"
# comment
# "character"
#
# > factors_all
# colname drivenType    dataType includedInAnalysis
# 1:                ICD-9-CM.15       data  continuous                 no
# 2:                      CC.15       data categorical
# 3:                ICD-9-CM.14       data categorical                 no
# 4:                      CC.14       data categorical
# 5:                ICD-9-CM.13       data categorical                 no
# ---
# 712: DXCCnoS_secondary_value179       data categorical                 no
# 713: DXCCnoS_secondary_value181       data categorical                 no
# 714: DXCCnoS_secondary_value182       data categorical                 no
# 715: DXCCnoS_secondary_value183       data categorical                 no
# 716: DXCCnoS_secondary_value184       data categorical                 no

#
# data_input[data_driven_factors_vec]
# NA_ratio <- sapply(data_input[,data_driven_factors_vec],function(x) round(mean(is.na(x)),digits=3) )
#---from above NA_ratio we can decide which column we need to replace NA with say 0.---#
catt("Start extracting response variables information!")
setkey(factors_all,drivenType)
response_variables <- factors_all[grep("response",drivenType,perl=T,ignore=T),][,colname]
catt("Start extracting knowledge_driven/data_driven factors information!")
knowledge_driven_factors <- factors_all[grep("knowledge",drivenType,perl=T,ignore=T),][,colname]
data_driven_factors <-  factors_all[grep("data",drivenType,perl=T,ignore=T),][,colname]
# id_variables <- factors_all[grep("id",drivenType,perl=T,ignore=T),][,colname]	# not used in linear model.
#---trim the leading or 
response_variables_vec <- sapply(response_variables,str_trim)
knowledge_driven_factors_vec <- sapply(knowledge_driven_factors,str_trim)
data_driven_factors_vec <- sapply(data_driven_factors,str_trim)
# id_variables <- sapply(id_variables,str_trim)

##
### 
#############################################
#------#
# check we have all names in the data.
#
stopifnot(response_variables_vec %in% colnames(data_input))
stopifnot(knowledge_driven_factors_vec %in% colnames(data_input))
stopifnot(data_driven_factors_vec %in% colnames(data_input))
catt("All extracted variables are validated to exsit in data_input!")
catt("We have response_variables are:",(response_variables_vec) )
catt("We have knowledge_driven_factors are:",knowledge_driven_factors_vec)
catt("We have data_driven_factors are:",head(data_driven_factors_vec),"..." )

#-------#
# subset the data_input to select only useful columns specified in factors_all.
data_input <- data_input[,factors_all$colname,with=F]
#-------#
# check dataType of each column, force continuous to be numeric col; categorical keep intact since it doesn't matter.
# data_input_backup <- data.table::copy(data_input)	# debugging usage.
# data_input <- data.table::copy(data_input_backup)	# debugging usage.
colname_continuous_dataType <-factors_all[grep("continuous",dataType,perl=T),colname]
# > data_input[,colname_continuous_dataType,with=F] %>% sapply(class)	# check data col obj type.
# NDX                        NPR
# "integer"                  "integer"
# acutePhysiologyScore                apacheScore
# "integer"                  "integer"
# predictedICUMortality         actualICUMortality
# "character"                "character"
# predictedICULOS               actualICULOS
# "character"                "character"
# predictedHospitalMortality    actualHospitalMortality
# "character"                "character"
# predictedHospitalLOS          actualHospitalLOS
# "character"                "character"
# N_CM                        AGE
# "numeric"                  "integer"
# LOS                     TOTCHG
# "integer"                  "numeric"

#---a subfunc to do dataType transformation (to numeric or factor depending on whether user expect it to be 'continuous' or 'categorical'), only take effect when the column is not of the wanted type yet.---
continuousVar_toNumeric <- function(variableName,data = data_input,annotation_table = factors_all) {
  setkey(annotation_table,colname )	
  data_type <- annotation_table[variableName,dataType]
  if (grepl("continuous",data_type,perl=T) & !(inherits( data_input[[variableName]], c("integer", "numeric") )) ) {
    col_value <- data[,(variableName),with=F] %>>% unlist %>>% as.numeric
    data[,variableName:=col_value,with=F]	# iif data type is continuous and the data col is not a in integer or numeric object type, this will make effect.
  }
  # ---for categorical we wan to make it to factor type.
  if (grepl("categorical",data_type,perl=T) & !(inherits( data_input[[variableName]], c("integer", "factor","numeric") )) ) {
    col_value <- data[,(variableName),with=F] %>>% unlist %>>% as.factor
    data[,variableName:=col_value,with=F]	# iif data type is continuous and the data col is not a in integer or numeric object type, this will make effect.
  }
  
  return(NULL)	# doesn't need to return, change in place.
}
#---obj type coerce execute---#
# coerce_temp <- tryCatch.W.E( sapply(colname_continuous_dataType,continuousVar_toNumeric,data = data_input,annotation_table = factors_all) )	
coerce_temp <- tryCatch.W.E( sapply( data_input %>>% colnames,continuousVar_toNumeric,data = data_input,annotation_table = factors_all) )	
if ( inherits(coerce_temp$value, "error" ) ) stop( sprintf("Error occurs at coercing continuous variables in provided data (%s) to be object of type 'numeric' or coercing categorical variable to be object of type 'factor', please clean the data if you haven't done so. Notice: do check the data if each variable(column) is of expected type, e.g. character, integer, numeric, etc. If in character, but you expect a continuous variable, you d better transform it to numeric value by yourself before providing it to this program. You want to also take care of those messy values like 'NULL','NA' in quotes within a character type column!", input_file) )
coerce_temp <- coerce_temp[["warning"]]	# the trimmed tryCatch warning return.
if (!is.null(coerce_temp))
  catt( sprintf("Warning occurs at coercing continuous/categorical variables in provided data (%s) to be object of type 'numeric'/'factor': %s", input_file,as.character(coerce_temp) ) )



#=================================================================================#
# if need to keep testing data out
if (exists("seed") )
  set.seed(seed)	# setting starting seed.
if (exists("ProportionHoldingTestDataset") ) {
  testDataset_index <- sample.int(nrow(data_input), ProportionHoldingTestDataset *nrow(data_input)  )	# ProportionHoldingTestDataset is a numeric
  data_inpu_testing <- data_input[testDataset_index]	# leave for testing
  data_input <- data_input[-testDataset_index]	# leave for training or kFoldCV.
}
# now we have data_inpu_testing
# and
#			  data_input, which can be further splitted for kFoldCV.
# Note: considering to apply a group-size proportional sampling.
#		e.g. for survival outcome, group on 'died', and sample proportionally.
# ###
# ##
# # for debugging
# setkeyv(NY_SID_2009_HFdata_refined,"died")
# data_input <- NY_SID_2009_HFdata_refined
# data_input_testing <- dplyr::sample_frac(NY_SID_2009_HFdata_refined,size=0.3)
# data_input <- dplyr::setdiff(data_input, data_input_testing)	# get the left part as new version of data_input.
# #
# ##
# ###
#=================================================================================#
# if need to manual kFoldCV, get the kFold_group_index ready, which contains the row index for each fold.
if (exists("kFoldCV") ) {
  total_index <- sample.int(nrow(data_input), nrow(data_input)  )	# already random perturbed index, e.g. 300,21,335,266,100,25...
  groupID <- length( total_index ) %>>% seq %:>% cut(., breaks = kFoldCV)
  # > split(total_index,groupID) %>>% unlist %>>% length
  # [1] 4064
  # > split(total_index,groupID) %>>% unlist %>>%
  # > total_index %>>% length
  # [1] 4064
  # > split(total_index,groupID) %>>% unlist %>>% anyDuplicated
  # [1] 0
  kFold_group_index <- split(total_index,groupID)
  stopifnot( length(kFold_group_index) == kFoldCV)
}




#=================================================================================#
# start load pre-specified model function.
if (exists("PrefeatureSelection_module") ) {
  featureSelection_module_loaded <- source(sprintf("%s/Module_file/%s.r",programRootPWD,PrefeatureSelection_module) )
  pre_feature_selection <- T	# the flag indicate whether we will do pre_feature_selection step before going to predictive_module step.
} else pre_feature_selection <- F
# start load pre-specified model function.
predictive_module_loaded <- source(sprintf("%s/Module_file/%s.r",programRootPWD,Predictive_module))
#
#
#--------------------------#
# filter on NA ratio > 0.05
NA_ratio <- sapply(data_input[,data_driven_factors_vec,with=F],function(x) round(mean(is.na(x)),digits=3) )
# NA_ratio
# 0 0.001 0.018 0.019
# 455     1     1     1
featureN_before_temp <- length(data_driven_factors_vec)
catt("Dropping data_driven_factors which have NA_ratio > 15%!")
# data_driven_factors_vec <- data_driven_factors_vec[NA_ratio <= 0.05]	# a further examine to guarantee NA ratio not too big.
data_driven_factors_vec <- data_driven_factors_vec[NA_ratio <= 0.15]	# a further examine to guarantee NA ratio not too big.
featureN_after_temp <- length(data_driven_factors_vec)
catt("Number of data_driven_factors before NA_ratio pruning:", featureN_before_temp)
catt("Number of data_driven_factors after NA_ratio pruning:", featureN_after_temp)

# programRootPWD <-  getwd() 	# for backup.
# invisible( ifelse(exists("output_folder_name"), yes = {dir.create(output_folder_name);setwd(output_folder_name)}, no = setwd("Output_file") ) )	# dont print the pwd which is the default behaviour of setwd.
#=================================1, pre feature selection============================#
if (pre_feature_selection) {	# need do pre_feature_selection step.
  catt("Now running PrefeatureSelection_module:",PrefeatureSelection_module)
  if (exists("Pf_m_config_file")) {	
    # featureSelection_call <- call(PrefeatureSelection_module,data = data_input, response_variables = response_variables_vec,    knowledge_driven_factors = knowledge_driven_factors_vec,data_driven_factors = data_driven_factors_vec, pvalue_cutoff = 0.05)
    tC_temp <- tryCatch.W.E(calls_pf_m <- extract_model_fromModelFile(sprintf("%s/Model_file/%s",programRootPWD,Pf_m_config_file)) )	# absolute path used here.
    if ( inherits(tC_temp$value, "error" ) ) stop("Error occurs at extract model from model file step, please check model_file syntax/path.")
    tC_temp <- tC_temp[["warning"]]	# the trimmed tryCatch warning return.
    if (!is.null(tC_temp))
      catt("Warning at extract model from model file step: ", as.character(tC_temp) )
    # raw calls may have typo, need to match.call in each real function.
    # predictiveModel_call <- call(predictive_module,data = data_input,response_variables = response_variables_vec, knowledge_driven_factors = knowledge_driven_factors_vec, data_driven_factors = data_driven_factors_vec, factors_annotation_table = factors_all)
    calls_pf_m_env <- list2env(calls_pf_m)	# transform to a environment for backup and further call. environment will save it against damage.
    #---#
    module_index <- match(sprintf(":%s",PrefeatureSelection_module), names(calls_pf_m) )	# hard match
    pre_feature_selectionResult <- eval(calls_pf_m[[module_index]][[1]])
  } else {	
    calls_pf_m <- NULL	# for compatiblity in further module.
    featureSelection_call <- call(PrefeatureSelection_module)	# use default argument values setup in function definition.
    # uniCorr()
    
    pre_feature_selectionResult <- eval(featureSelection_call)
    
    # DXCCS_secondary_value5
    # 0     1
    # 63934   646
    # but in linear model, it is further filtered for NAs in other variables, so eventually becomes all 0.
  }
  data_driven_factors_vec <- pre_feature_selectionResult$features	# update the data_driven_factors_vec with fewer candidates passing this step.
  # catt("We have ", length(data_driven_factors_vec), " data-driven variables left after 'pre-feature selection' module")
  save(pre_feature_selectionResult,file=sprintf("%s_pre_feature_selectionResult_by_%s.Rdata",prefix,PrefeatureSelection_module) )
  catt("Number of data_driven_factors after pre_feature_selection:", length(data_driven_factors_vec))
}


#================================2, predictive model====================================#
# Browse[2]> data[,sapply(.SD,function(x)sum(is.na(x))),.SDcols=c(knowledge_driven_factors_vec,data_driven_factors_vec)] %>>% table

# 0   69  223 1139 1198
# 457    1    1    1    1
# originalN <- nrow(data_input)
#---#
# extract the calls from corresponding model file for predictive model.
catt("Now running Predictive_module:",Predictive_module)
if (exists("p_m_config_file")) {
  tC_temp <- tryCatch.W.E(calls_pm <- extract_model_fromModelFile(sprintf("%s/Model_file/%s",programRootPWD,p_m_config_file)) )	
  if ( inherits(tC_temp$value, "error" ) ) stop("Error occurs at extract model from model file step, please check model_file syntax/path.")
  #
  tC_temp <- tC_temp[["warning"]]	# the trimmed tryCatch warning return.
  if (!is.null(tC_temp))
    catt("Warning at 'extract_model_fromModelFile' to 'calls_pm' step: ", as.character(tC_temp) )
  # raw calls may have typo, need to match.call in each real function.
  # predictiveModel_call <- call(Predictive_module,data = data_input,response_variables = response_variables_vec, knowledge_driven_factors = knowledge_driven_factors_vec, data_driven_factors = data_driven_factors_vec, factors_annotation_table = factors_all)
  calls_pm_env <- list2env(calls_pm)	# transform to a environment for backup and further call. environment will save it against damage.
  #---#
  module_index <- match(sprintf(":%s",Predictive_module), names(calls_pm) )	# hard match
  predictiveResult <- eval(calls_pm[[module_index]][[1]])
} else {
  calls_pm <- NULL	# for compatiblity in further module.
  predictiveModel_call <- call(Predictive_module)
  predictiveResult <- eval(predictiveModel_call)	# use default function definition setup.
  # not recommended.
  # recomend use p_m_config_file
}
save(predictiveResult,file=sprintf("%s_predictiveResult_by_%s.Rdata",prefix,Predictive_module) )


#==================================================================================#
# space for testing model performance on hold out data_inpu_testing
# each predictive_module need a corresponding evaluate_predict_module.
# 
#==================================================================================#


message("========================================================" );
catt("Finished!" )
catt("End at ", date() )
catt("Please check log file under program root and all result outputs in", outputPWD )
#---------------#
# end parallel_scheme
if (exists("cl")) stopCluster(cl)	# only cl need to be stopped.
if (exists("sc")) sparkR.stop() # stop Spark cluster if any.
#---------------#
# end logging system.
sink()
sink(type="message")
close(log_connection)
